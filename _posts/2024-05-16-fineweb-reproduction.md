# ä»‹ç»

FineWebåŒ…å«è¶…è¿‡15TB tokençš„æ¸…æ´—å¹¶åˆ å†—åçš„è‹±è¯­Webæ•°æ®é›†ã€‚
æ˜¯RefinedWebçš„å¼€æºå¤ç°ï¼ŒRefinedWebæ•°æ®é›†å‘äº†è®ºæ–‡ä½†æ˜¯æœªå¼€æºã€‚
æ•°æ®å¤„ç†ç®¡çº¿é¢å‘LLMæ€§èƒ½ä¼˜åŒ–ã€‚
è¿™ä¸ªæ•°æ®é›†æ¥å…¥äº†`datatrove`åº“ï¼Œ
ä»–ä»¬å¼€å‘çš„å¤§è§„æ¨¡æ•°æ®å¤„ç†åº“ã€‚

# å¤ç°

## åˆ†æCommon Crawl

æ€»å…±æœ‰99ä¸ªArchive Crawlï¼Œæ¯ä¸ªArchive Crawlæœ‰100ä¸ªSegmentï¼Œå› æ­¤å…±è®¡æœ‰9900ä¸ªSegmentã€‚
å¯¹äºWARCï¼Œæ¯ä¸ªSegmentå¤§çº¦æœ‰800ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ç”¨gzç®—æ³•å‹ç¼©ï¼Œå¤§çº¦æ¯ä¸ªå‹ç¼©åæœ‰1.2GBã€‚

å› æ­¤æ•´ä¸ªCommon Crawlæœ‰8910000ä¸ªæ–‡ä»¶ï¼ŒæŒ‰ç…§æ¯ä¸ªæ–‡ä»¶ç»è¿‡finewebæ¸…æ´—åçš„å¤§å°ä¸º18MBï¼Œæ€»å¤§å°ä¸º160TBï¼Œå’Œ15T Tokenå·®ä¸å¤šèƒ½å¯¹ä¸Šäº†ã€‚

```bash
for s in $(aws s3 ls s3://commoncrawl/crawl-data/ | grep CC-MAIN- | tr -s ' ' | cut -d ' ' -f 3 | sed 's#/##g'); do
    echo $s
    aws s3 cp s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segment.paths.gz ${s}_segment.paths.gz
done
gunzip *.paths.gz
cat *.paths | wc -l
```

## å¤ç°datatrove/process_common_crawl_dump

æ³¨æ„ï¼šä½¿ç”¨S3å¤ç°å¤±è´¥ï¼Œbogo3æŠ¥é”™APIè°ƒç”¨é¢‘ç‡å¤ªé«˜ã€‚è€Œä¸”å†™å…¥S3ä¼šçœ‹ä¸åˆ°ä¸­é—´ç»“æœï¼Œæœ¬åœ°æ¨¡å¼å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„ä¸­é—´ç»“æœã€‚

```bash
sudo apt install aws
# aws s3 mb s3://fineweb-reproduction-20240516
mkdir -p /root/fineweb/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc
cd /root/fineweb/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc
# aws s3 sync s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc .
aws s3 cp s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc/CC-MAIN-20230606214755-20230607004755-00000.warc.gz .

python3 -m venv pyvenv-fineweb
source pyvenv-fineweb/bin/activate
pip install datatrove[io,processing,s3]
wget https://raw.githubusercontent.com/huggingface/datatrove/main/examples/process_common_crawl_dump.py
sed -i 's#s3://some_s3_bucket/base_processing/#/root/fineweb/base_processing/#g' process_common_crawl_dump.py
sed -i 's#from datatrove.executor.slurm import SlurmPipelineExecutor#from datatrove.executor import LocalPipelineExecutor#g' process_common_crawl_dump.py
sed -i 's#SlurmPipelineExecutor#LocalPipelineExecutor#g' process_common_crawl_dump.py
sed -i 's#Trafilatura(favour_precision=True)#Trafilatura(favour_precision=True, timeout=10.0)#g' process_common_crawl_dump.py
python -c "import nltk; nltk.download('punkt')"
python process_common_crawl_dump.py CC-MAIN-2023-23
# aws s3 ls s3://fineweb-reproduction-20240516/base_processing/
```

ç»“æœå¦‚ä¸‹ï¼š
1. å¯è§ç“¶é¢ˆåœ¨Trafilaturaä¸Šï¼Œå æ®äº†91%çš„æ—¶é—´ã€‚å› ä¸ºTrafilaturaè¿‡æ»¤æ‰äº†å¤§é‡æ–‡æœ¬ï¼Œè¾“å…¥æ–‡æœ¬å¤§å°ä¸º4.72GBï¼Œè€Œè¾“å‡ºæ–‡æœ¬å¤§å°ä»…ä¸º86MBï¼Œä»…ä¿ç•™1.83%çš„æ–‡æœ¬ã€‚
2. å¤„ç†1.2GBå‹ç¼©åçš„CCæ•°æ®é›†ï¼Œè¿™æ˜¯CC-MAIN-2023-23çš„Crawl Archiveï¼ˆå…±æœ‰99ä¸ªArchiveï¼‰ä¸‹çš„1685224653183.5çš„Segmentï¼ˆå…±æœ‰100ä¸ªSegmentï¼‰ï¼Œå•ä¸ªçº¿ç¨‹ï¼Œç”¨æ—¶1156ç§’ï¼Œä¹Ÿå°±æ˜¯1.06MB/sã€‚

```
2024-05-17 06:39:59.728 | SUCCESS  | datatrove.executor.local:run:146 -

ğŸ“‰ğŸ“‰ğŸ“‰ Stats: All 1 tasks ğŸ“‰ğŸ“‰ğŸ“‰

Total Runtime: 19 minutes and 16 seconds

ğŸ“– - READER: ğŸ•· Warc
    Runtime: (2.09%) 24 seconds [0.20 millisecondsÂ±0.66 milliseconds/doc]
    Stats: {input_files: 1, doc_len: 4740085439 [min=1, max=1048576, 138749.10Â±183114/doc], documents: 34162 [34162.00/input_file]}
ğŸ”» - FILTER: ğŸ˜ˆ Url-filter
    Runtime: (0.49%) 5 seconds [0.17 millisecondsÂ±11.14 milliseconds/doc]
    Stats: {total: 34163, forwarded: 33963, doc_len: 4720996535 [min=1, max=1048576, 139004.11Â±183434/doc], dropped: 200, dropped_domain: 101, dropped_hard_blacklisted: 75, dropped_blacklisted_subword: 17, dropped_soft_blacklisted: 6, dropped_subdomain: 1}
ğŸ›¢ - EXTRAC: â› Trafilatura
    Runtime: (91.08%) 17 minutes and 33 seconds [31.02 millisecondsÂ±58.37 milliseconds/doc]
    Stats: {total: 33963, forwarded: 32550, doc_len: 86438862 [min=1, max=601304, 2655.57Â±13256/doc], dropped: 1413}
ğŸ”» - FILTER: ğŸŒ Language ID
    Runtime: (2.01%) 23 seconds [0.71 millisecondsÂ±2.47 milliseconds/doc]
    Stats: {total: 32550, dropped: 20766, forwarded: 11784, doc_len: 30116540 [min=8, max=292206, 2555.71Â±6959/doc]}
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition
    Runtime: (2.70%) 31 seconds [2.65 millisecondsÂ±6.70 milliseconds/doc]
    Stats: {total: 11784, forwarded: 8545, doc_len: 21903496 [min=8, max=131305, 2563.31Â±4958/doc], dropped: 3239, dropped_dup_line_frac: 1387, dropped_duplicated_5_n_grams: 336, dropped_top_3_gram: 210, dropped_top_2_gram: 694, dropped_duplicated_6_n_grams: 24, dropped_duplicated_9_n_grams: 16, dropped_top_4_gram: 383, dropped_duplicated_10_n_grams: 27, dropped_dup_line_char_frac: 133, dropped_duplicated_7_n_grams: 14, dropped_duplicated_8_n_grams: 15}
ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality
    Runtime: (1.44%) 16 seconds [1.95 millisecondsÂ±3.32 milliseconds/doc]
    Stats: {total: 8545, forwarded: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc], dropped: 2474, dropped_gopher_too_many_end_ellipsis: 240, dropped_gopher_short_doc: 1095, dropped_gopher_below_alpha_threshold: 1108, dropped_gopher_enough_stop_words: 20, dropped_gopher_too_many_ellipsis: 2, dropped_gopher_above_avg_threshold: 1, dropped_gopher_too_many_hashes: 2, dropped_gopher_too_many_bullets: 3, dropped_gopher_below_avg_threshold: 3}
ğŸ’½ - WRITER: ğŸ¿ Jsonl
    Runtime: (0.18%) 2 seconds [0.34 millisecondsÂ±0.46 milliseconds/doc]
    Stats: {XXXXX.jsonl.gz: 6071, total: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc]}
```

## å¤ç°Fineweb

```bash
wget https://raw.githubusercontent.com/huggingface/datatrove/main/examples/fineweb.py
sed -i 's/#.*//g' fineweb.py
sed -i 's#from datatrove.executor.slurm import SlurmPipelineExecutor#from datatrove.executor import LocalPipelineExecutor#g' fineweb.py
sed -i 's#CC-MAIN-2O23-5O#CC-MAIN-2023-23#g' fineweb.py
sed -i 's#s3://some_s3_bucket#/root/fineweb#g' fineweb.py
sed -i 's#SlurmPipelineExecutor#LocalPipelineExecutor#g' fineweb.py
sed -i 's#job_name=.*,##g' fineweb.py
sed -i 's#s3://commoncrawl/crawl-data#/root/fineweb/crawl-data#g' fineweb.py
sed -i 's#Trafilatura(favour_precision=True)#Trafilatura(favour_precision=True, timeout=10.0)#g' fineweb.py
sed -i 's#tasks=.*,#tasks=1,#g' fineweb.py
sed -i 's#time=".*",##g' fineweb.py
sed -i 's#slurm_logs_folder=.*",##g' fineweb.py
sed -i 's#randomize_start=.*,##g' fineweb.py
sed -i 's#mem_per_cpu_gb=.*,##g' fineweb.py
sed -i 's#partition=.*,##g' fineweb.py
sed -i 's#logging_dir=.*,##g' fineweb.py
sed -i 's#cpus_per_task=.*,##g' fineweb.py
sed -i 's#depends=.*,##g' fineweb.py
sed -i 's#^stage4.run()#stage1.run(); stage2.run(); stage3.run(); stage4.run()#g' fineweb.py
python -c "import nltk; nltk.download('punkt')"
/usr/bin/time -v python fineweb.py 2>&1 | tee fineweb.log
```

ç»“æœå¦‚ä¸‹

```
--- ğŸ› ï¸ PIPRLINEWğŸ›                                                                                                   [66/1812]
ğŸ“– - READER: ğŸ•· Warc                                                                                                         ğŸ”» - FILTER: ğŸ˜ˆ Url-filter                                                                                                  ğŸ›¢ - EXTRAC: â› Trafilatura
ğŸ”» - FILTER: ğŸŒ Language ID
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition                                                                                           ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality                                                                                              ğŸ”» - FILTER: â›° C4 Quality                                                                                                   ğŸ”» - FILTER: ğŸ· FineWeb Quality
ğŸ’½ - WRITER: ğŸ¿ Jsonl
2024-05-17 07:13:38.553 | INFO     | datatrove.pipeline.readers.base:read_files_shard:193 - Reading input file 1685224653183.5/warc/CC-MAIN-20230606214755-20230607004755-00000.warc.gz                                                                 2024-05-17 07:13:43.012 | WARNING  | datatrove.pipeline.readers.base:get_document_from_dict:93 - Found document without text, skipping. Is your `text_key` ("text") correct?                                                                            2024-05-17 07:33:20.415 | SUCCESS  | datatrove.executor.base:_run_for_rank:85 - Processing done for rank=0
2024-05-17 07:33:20.417 | INFO     | datatrove.executor.base:_run_for_rank:91 -
                                                                                                                            ğŸ“‰ğŸ“‰ğŸ“‰ Stats: Task 0 ğŸ“‰ğŸ“‰ğŸ“‰
                                                                                                                            Total Runtime: 19 minutes and 27 seconds
                                                                                                                            ğŸ“– - READER: ğŸ•· Warc
    Runtime: (2.08%) 24 seconds [0.21 millisecondsÂ±0.66 milliseconds/doc]
    Stats: {input_files: 1, doc_len: 4740085439 [min=1, max=1048576, 138749.10Â±183114/doc], documents: 34162 [34162.00/input_file]}                                                                                                                     ğŸ”» - FILTER: ğŸ˜ˆ Url-filter                                                                                                      Runtime: (0.49%) 5 seconds [0.17 millisecondsÂ±11.06 milliseconds/doc]
    Stats: {total: 34163, forwarded: 33963, doc_len: 4720996535 [min=1, max=1048576, 139004.11Â±183434/doc], dropped: 200, dropped_domain: 101, dropped_hard_blacklisted: 75, dropped_blacklisted_subword: 17, dropped_soft_blacklisted: 6, dropped_subdomain: 1}                                                                                                                    ğŸ›¢ - EXTRAC: â› Trafilatura                                                                                                       Runtime: (90.03%) 17 minutes and 31 seconds [30.96 millisecondsÂ±57.26 milliseconds/doc]
    Stats: {total: 33963, forwarded: 32550, doc_len: 86438862 [min=1, max=601304, 2655.57Â±13256/doc], dropped: 1413}
ğŸ”» - FILTER: ğŸŒ Language ID                                                                                                     Runtime: (1.99%) 23 seconds [0.72 millisecondsÂ±2.44 milliseconds/doc]                                                       Stats: {total: 32550, dropped: 20766, forwarded: 11784, doc_len: 30116540 [min=8, max=292206, 2555.71Â±6959/doc]}
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition
    Runtime: (2.66%) 31 seconds [2.64 millisecondsÂ±6.71 milliseconds/doc]
    Stats: {total: 11784, forwarded: 8545, doc_len: 21903496 [min=8, max=131305, 2563.31Â±4958/doc], dropped: 3239, dropped_dup_line_frac: 1387, dropped_duplicated_5_n_grams: 336, dropped_top_3_gram: 210, dropped_top_2_gram: 694, dropped_duplicated_6_n_grams: 24, dropped_duplicated_9_n_grams: 16, dropped_top_4_gram: 383, dropped_duplicated_10_n_grams: 27, dropped_dup_line_char_frac: 133, dropped_duplicated_7_n_grams: 14, dropped_duplicated_8_n_grams: 15}
ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality                                                                                                  Runtime: (1.42%) 16 seconds [1.94 millisecondsÂ±3.30 milliseconds/doc]
    Stats: {total: 8545, forwarded: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc], dropped: 2474, dropped_gopher_too_many_end_ellipsis: 240, dropped_gopher_short_doc: 1095, dropped_gopher_below_alpha_threshold: 1108, dropped_gopher_enough_stop_words: 20, dropped_gopher_too_many_ellipsis: 2, dropped_gopher_above_avg_threshold: 1, dropped_gopher_too_many_hashes: 2, dropped_gopher_too_many_bullets: 3, dropped_gopher_below_avg_threshold: 3}
ğŸ”» - FILTER: â›° C4 Quality
    Runtime: (0.32%) 3 seconds [0.62 millisecondsÂ±1.02 milliseconds/doc]
    Stats: {total: 6071, line-total: 123281, line-kept: 97170, dropped: 602, dropped_too_few_sentences: 573, line-filter-too_few_words: 25349, forwarded: 5469, doc_len: 17992639 [min=149, max=123528, 3289.93Â±5383/doc], line-filter-policy: 641, dropped_curly_bracket: 22, line-filter-javascript: 92, dropped_lorem_ipsum: 7}
ğŸ”» - FILTER: ğŸ· FineWeb Quality
    Runtime: (0.85%) 9 seconds [1.81 millisecondsÂ±2.58 milliseconds/doc]
    Stats: {total: 5469, dropped: 647, dropped_line_punct_ratio: 336, forwarded: 4822, doc_len: 15898156 [min=151, max=99375, 3297.00Â±4732/doc], dropped_short_line_ratio: 86, dropped_char_dup_ratio: 225}
ğŸ’½ - WRITER: ğŸ¿ Jsonl
    Runtime: (0.15%) 1 second [0.37 millisecondsÂ±0.45 milliseconds/doc]
    Stats: {XXXXX.jsonl.gz: 4822, total: 4822, doc_len: 15898156 [min=151, max=99375, 3297.00Â±4732/doc]}
2024-05-17 07:33:20.423 | SUCCESS  | datatrove.executor.local:run:146 -

ğŸ“‰ğŸ“‰ğŸ“‰ Stats: All 1 tasks ğŸ“‰ğŸ“‰ğŸ“‰

Total Runtime: 19 minutes and 27 seconds

        Command being timed: "python fineweb.py"
        User time (seconds): 1177.85
        System time (seconds): 2.05
        Percent of CPU this job got: 99%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 19:42.98
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 1029440
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 1
        Minor (reclaiming a frame) page faults: 342151
        Voluntary context switches: 71803
        Involuntary context switches: 3084
        Swaps: 0
        File system inputs: 0
        File system outputs: 84208
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 1
```

# æ€§èƒ½åˆ†æ

ç»™`WarcReader`åŠ ä¸€ä¸ª`limit=1000`å³å¯åœ¨26ç§’å†…å‡ºç»“æœã€‚

```bash
python -m cProfile -o process_common_crawl_dump.cProfile -s cumtime process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.cProfile
```

çœ‹ä¸Šå»cPythonå¯¹å¤šçº¿ç¨‹çš„æ”¯æŒæœ‰é™ï¼Œ[extractorä½¿ç”¨äº†ThreadPoolExecutoræ¥åšåŸºäºå¤šçº¿ç¨‹çš„çº¿ç¨‹æ± ](https://github.com/huggingface/datatrove/blob/main/src/datatrove/pipeline/extractors/base.py#L48)ã€‚

å°è¯•`yaapi`ï¼Œå¯ä»¥è‡ªåº•å‘ä¸Šçœ‹åˆ°å…·ä½“å“ªäº›è°ƒç”¨æ˜¯çƒ­ç‚¹ã€‚ä¸è¿‡å®ƒå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåœ¨ä½¿ç”¨yappiå¯¹Cythonæ¨¡å—è¿›è¡Œåˆ†ææ—¶ï¼ŒCythonå‡½æ•°çš„è°ƒç”¨ç»Ÿè®¡é€šå¸¸ä¼šå½’å…¥è°ƒç”¨å®ƒä»¬çš„Pythonå‡½æ•°ã€‚

```bash
pip install yappi
python -m yappi -c cpu -o process_common_crawl_dump.yappi -f pstat process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.yappi
# sort tottime
# stats 100

# è‹¥æ²¡æœ‰-bï¼Œåˆ™ä¸ä¼šå°†builtinå‡½æ•°è€ƒè™‘è¿›å»ï¼Œæ‰€ä»¥æ‰“å¼€-bè¿›ä¸€æ­¥çœ‹çœ‹å“ªäº›builtinæ˜¯æœ€éœ€è¦ä¼˜åŒ–çš„
python -m yappi -c cpu -b -o process_common_crawl_dump.yappi.withBuiltins -f pstat process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.yappi.withBuiltins
# sort tottime
# stats 100
```

ä½¿ç”¨`py-spy`ç”»ç«ç„°å›¾ï¼Œå¯ä»¥è‡ªé¡¶å‘ä¸‹åšæ€§èƒ½åˆ†æã€‚[py-spyé»˜è®¤æ˜¯CPUæ¨¡å¼ï¼Œåœ¨åŠ ä¸Š--idleå‘½ä»¤ä¹‹åæ˜¯Wallæ¨¡å¼](https://github.com/benfred/py-spy/issues/458)ï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹CPUæ¨¡å¼ï¼š


```bash
pip install py-spy
py-spy record -o process_common_crawl_dump.pyspy.svg -f flamegraph --rate 100 -- python process_common_crawl_dump.py CC-MAIN-2023-23
```

å°è¯•`line_profiler`ï¼š

```bash
pip install line_profiler
vi pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
# åœ¨def prune_unwanted_nodesä¹‹å‰å¢åŠ  @line_profiler.profile
LINE_PROFILE=1 python process_common_crawl_dump.py CC-MAIN-2023-23
python -m line_profiler -rtmz profile_output.lprof
```

è¾“å‡ºç»“æœï¼š
```
 11.13 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
Wrote profile results to profile_output.txt
Wrote profile results to profile_output_2024-05-19T081609.txt
Wrote profile results to profile_output.lprof
To view details run:
python -m line_profiler -rtmz profile_output.lprof
```

è¿›ä¸€æ­¥æŸ¥çœ‹
```
Timer unit: 1e-06 s

Total time: 11.1257 s
File: /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
Function: prune_unwanted_nodes at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @line_profiler.profile
    91                                           def prune_unwanted_nodes(tree, nodelist, with_backup=False):
    92                                               '''Prune the HTML tree by removing unwanted sections.'''
    93     11665       3130.6      0.3      0.0      if with_backup:
    94      1577      25789.5     16.4      0.2          old_len = len(tree.text_content())  # ' '.join(tree.itertext())
    95      1577      77741.7     49.3      0.7          backup = deepcopy(tree)
    96                                           
    97     27462       6424.3      0.2      0.1      for expression in nodelist:
    98     54290   10884183.9    200.5     97.8          for subtree in expression(tree):
    99                                                       # preserve tail text from deletion
   100     38493       8026.8      0.2      0.1              if subtree.tail is not None:
   101     28309      10939.3      0.4      0.1                  prev = subtree.getprevious()
   102     28309       3699.4      0.1      0.0                  if prev is None:
   103     17852       6883.3      0.4      0.1                      prev = subtree.getparent()
   104     28309       3616.2      0.1      0.0                  if prev is not None:
   105                                                               # There is a previous node, append text to its tail
   106     28309      23310.3      0.8      0.2                      prev.tail = " ".join([prev.tail, subtree.tail]) if prev.tail else subtree.tail
   107                                                       # remove the node
   108     38493      52627.2      1.4      0.5              subtree.getparent().remove(subtree)
   109                                           
   110     11665       1707.4      0.1      0.0      if not with_backup:
   111     10088       1384.7      0.1      0.0          return tree
   112                                           
   113      1577      14788.8      9.4      0.1      new_len = len(tree.text_content())
   114                                               # todo: adjust for recall and precision settings
   115      1577       1195.3      0.8      0.0      if new_len > old_len/7:
   116      1444        198.0      0.1      0.0          return tree
   117       133         17.6      0.1      0.0      return backup

 11.13 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
 ```

 æ¨æµ‹æ˜¯`expression(tree)`è°ƒç”¨æ—¶é—´æœ€é•¿ï¼ŒæŠŠå®ƒä»forå¾ªç¯é‡Œæ‹†å¼€æ¥åè¿›ä¸€æ­¥åˆ†æï¼Œæœç„¶`XPath`æ±‚å€¼æ˜¯æœ€æ…¢çš„ï¼Œå äº†97.1%çš„æ—¶é—´ã€‚

 ```
 Timer unit: 1e-06 s

Total time: 11.1063 s
File: /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
Function: prune_unwanted_nodes at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @line_profiler.profile
    91                                           def prune_unwanted_nodes(tree, nodelist, with_backup=False):
    92                                               '''Prune the HTML tree by removing unwanted sections.'''
    93     11665       3206.3      0.3      0.0      if with_backup:
    94      1577      25812.4     16.4      0.2          old_len = len(tree.text_content())  # ' '.join(tree.itertext())
    95      1577      78249.5     49.6      0.7          backup = deepcopy(tree)
    96                                           
    97     27462       6485.1      0.2      0.1      for expression in nodelist:
    98     15797   10848708.6    686.8     97.7          subtrees = expression(tree)
    99     54290      13832.9      0.3      0.1          for subtree in subtrees:
   100                                                       # preserve tail text from deletion
   101     38493       7947.7      0.2      0.1              if subtree.tail is not None:
   102     28309      11156.7      0.4      0.1                  prev = subtree.getprevious()
   103     28309       3635.4      0.1      0.0                  if prev is None:
   104     17852       7424.6      0.4      0.1                      prev = subtree.getparent()
   105     28309       3392.5      0.1      0.0                  if prev is not None:
   106                                                               # There is a previous node, append text to its tail
   107     28309      23303.6      0.8      0.2                      prev.tail = " ".join([prev.tail, subtree.tail]) if prev.tail else subtree.tail
   108                                                       # remove the node
   109     38493      52844.9      1.4      0.5              subtree.getparent().remove(subtree)
   110                                           
   111     11665       1721.9      0.1      0.0      if not with_backup:
   112     10088       1288.3      0.1      0.0          return tree
   113                                           
   114      1577      15838.1     10.0      0.1      new_len = len(tree.text_content())
   115                                               # todo: adjust for recall and precision settings
   116      1577       1234.6      0.8      0.0      if new_len > old_len/7:
   117      1444        200.1      0.1      0.0          return tree
   118       133         17.2      0.1      0.0      return backup

 11.11 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
 ```

## åˆ†æç»“æœ

é€šè¿‡`yappi`çš„ç»“æœå‘ç°`prune_unwanted_nodes`åœ¨Pythonè§£é‡Šå™¨çš„éƒ¨åˆ†å ç”¨äº†ä¸å°‘æ—¶é—´ï¼Œ
ä½†æ˜¯ç”±äºå†å¾€ä¸‹æ˜¯ç”¨Cythonå®ç°çš„ï¼Œæ²¡æ³•å†è¿›ä¸€æ­¥åˆ†è§£äº†ã€‚
å°è¯•ç”¨line_profilerè¿›è¡Œåˆ†æã€‚

æ€»å¾—æ¥è¯´ï¼Œå¯ä»¥ç”¨`yappi`/`py-spy`å®šä½åˆ°çƒ­ç‚¹Pythonå‡½æ•°ï¼Œå¦‚æœèƒ½æ‰¾åˆ°çƒ­ç‚¹å‡½æ•°ï¼Œå¯ä»¥å†ç”¨`line_profiler`è¿›ä¸€æ­¥ç»†åŒ–çƒ­ç‚¹æƒ…å†µã€‚

# ç›¸å…³å¼€æºæ•°æ®é›†

1. [RefinedWeb](https://huggingface.co/papers/2306.01116)ï¼šæœªå¼€æº
2. [C4](https://www.tensorflow.org/datasets/catalog/c4)ï¼šè°·æ­ŒåŸºäºCommon Crawlæ¸…æ´—å‡ºæ¥çš„æ•°æ®é›†
3. 

# ç›¸å…³å¼€æºåº“

1. [datatrove](https://github.com/huggingface/datatrove)ï¼šHugging Faceå®˜æ–¹æ¨å‡ºçš„åŸºäºSlurmçš„æ•°æ®å¤„ç†åº“ã€‚
2. 

# Hugging Faceå®˜æ–¹çš„åº“

1. [transformers](https://github.com/huggingface/transformers)ï¼šä¸šç•Œé¢†å…ˆçš„é¢å‘JAXã€PyTorchã€TensorFlowçš„æœºå™¨å­¦ä¹ ã€‚
2. [diffusers](https://github.com/huggingface/diffusers)ï¼šä¸šç•Œé¢†å…ˆçš„ç”¨äºå›¾åƒã€éŸ³é¢‘ç”Ÿæˆçš„æ¨¡å‹ï¼ˆåŸºäºPyTorchå’ŒFLAXï¼‰ã€‚
3. [dataset](https://github.com/huggingface/datasets)ï¼šé¢å‘æœºå™¨å­¦ä¹ è®­ç»ƒçš„æ•°æ®é›†ï¼Œä»¥åŠæ˜“ç”¨ã€é«˜æ•ˆçš„æ•°æ®å¤„ç†å·¥å…·ã€‚
4. [peft](https://github.com/huggingface/peft)ï¼šä¸šç•Œé¢†å…ˆçš„å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ŒPEFTæ–¹æ³•åªéœ€è¦fine-tuningä¸€å°éƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„æ¨¡å‹å‚æ•°ã€‚
5. [accelerate](https://github.com/huggingface/accelerate)ï¼šè¿è¡Œã€è®­ç»ƒã€ä½¿ç”¨PyTorchæ¨¡å‹åœ¨ä»»ä½•è®¾å¤‡ã€åˆ†å¸ƒå¼é…ç½®ä¸‹ã€‚è‡ªåŠ¨æ–½åŠ æ··åˆç²¾åº¦ã€‚
6. [optimum](https://github.com/huggingface/optimum)ï¼šåŠ é€ŸTransformerä¸Diffusersçš„è®­ç»ƒä¸æ¨ç†ï¼Œé¢å‘å„ç§ç¡¬ä»¶ä¼˜åŒ–å·¥å…·ã€‚

