# ä»‹ç»

FineWebåŒ…å«è¶…è¿‡15TB tokençš„æ¸…æ´—å¹¶åˆ å†—åçš„è‹±è¯­Webæ•°æ®é›†ã€‚
æ˜¯RefinedWebçš„å¼€æºå¤ç°ï¼ŒRefinedWebæ•°æ®é›†å‘äº†è®ºæ–‡ä½†æ˜¯æœªå¼€æºã€‚
æ•°æ®å¤„ç†ç®¡çº¿é¢å‘LLMæ€§èƒ½ä¼˜åŒ–ã€‚
è¿™ä¸ªæ•°æ®é›†æ¥å…¥äº†`datatrove`åº“ï¼Œ
ä»–ä»¬å¼€å‘çš„å¤§è§„æ¨¡æ•°æ®å¤„ç†åº“ã€‚

## ä¸LLaMA 3çš„å…³ç³»

å‚è§[Introducing Meta Llama 3: The most capable openly available LLM to date
](https://ai.meta.com/blog/meta-llama-3/)ï¼Œå…¶ä¸­æåˆ°å®ƒä»¬ä»å„ç§å…¬å¼€æ•°æ®æºæ”¶é›†æ•°æ®ï¼Œé€šè¿‡ä¸€ç³»åˆ—è¿‡æ»¤ç®¡çº¿ï¼ˆå¯å‘å¼è¿‡æ»¤å™¨ã€NSFWè¿‡æ»¤å™¨ã€è¯­ä¹‰åˆ å†—ã€æ–‡æœ¬è´¨é‡åˆ†ç±»å™¨ï¼‰å°†æ•°æ®æ¸…æ´—ã€‚ç‰¹åˆ«æåˆ°ï¼Œ**ç”¨äºè®­ç»ƒæ–‡æœ¬è´¨é‡åˆ†ç±»å™¨çš„è®­ç»ƒæ•°æ®ç”±Llama 2æ¨¡å‹ç”Ÿæˆï¼‰**ã€‚

ASIDEï¼šåœ¨Instruction Fine-tuningé˜¶æ®µï¼Œé€šè¿‡SFTï¼ˆSupervised Fine-tuningï¼‰ã€æ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰ã€PPOï¼ˆProximal Policy Optimizationï¼‰ã€DPOï¼ˆDirect Preference Optimizationï¼‰ç­‰æŠ€æœ¯æ¥åšåè®­ç»ƒå¤„ç†ã€‚

# å¤ç°

## åˆ†æCommon Crawl

æ€»å…±æœ‰99ä¸ªArchive Crawlï¼Œæ¯ä¸ªArchive Crawlæœ‰100ä¸ªSegmentï¼Œå› æ­¤å…±è®¡æœ‰9900ä¸ªSegmentã€‚
å¯¹äºWARCï¼Œæ¯ä¸ªSegmentå¤§çº¦æœ‰800ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ç”¨gzç®—æ³•å‹ç¼©ï¼Œå¤§çº¦æ¯ä¸ªå‹ç¼©åæœ‰1.2GBã€‚

å› æ­¤æ•´ä¸ªCommon Crawlæœ‰8910000ä¸ªæ–‡ä»¶ï¼ŒæŒ‰ç…§æ¯ä¸ªæ–‡ä»¶ç»è¿‡finewebæ¸…æ´—åçš„å¤§å°ä¸º18MBï¼Œæ€»å¤§å°ä¸º160TBï¼Œå’Œ15T Tokenå·®ä¸å¤šèƒ½å¯¹ä¸Šäº†ã€‚

```bash
for s in $(aws s3 ls s3://commoncrawl/crawl-data/ | grep CC-MAIN- | tr -s ' ' | cut -d ' ' -f 3 | sed 's#/##g'); do
    echo $s
    aws s3 cp s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segment.paths.gz ${s}_segment.paths.gz
done
gunzip *.paths.gz
cat *.paths | wc -l
```

## å¤ç°datatrove/process_common_crawl_dump

æ³¨æ„ï¼šä½¿ç”¨S3å¤ç°å¤±è´¥ï¼Œbogo3æŠ¥é”™APIè°ƒç”¨é¢‘ç‡å¤ªé«˜ã€‚è€Œä¸”å†™å…¥S3ä¼šçœ‹ä¸åˆ°ä¸­é—´ç»“æœï¼Œæœ¬åœ°æ¨¡å¼å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„ä¸­é—´ç»“æœã€‚

```bash
sudo apt install aws
# aws s3 mb s3://fineweb-reproduction-20240516
mkdir -p /root/fineweb/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc
cd /root/fineweb/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc
# aws s3 sync s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc .
aws s3 cp s3://commoncrawl/crawl-data/CC-MAIN-2023-23/segments/1685224653183.5/warc/CC-MAIN-20230606214755-20230607004755-00000.warc.gz .

python3 -m venv pyvenv-fineweb
source pyvenv-fineweb/bin/activate
pip install datatrove[io,processing,s3]
wget https://raw.githubusercontent.com/huggingface/datatrove/main/examples/process_common_crawl_dump.py
sed -i 's#s3://some_s3_bucket/base_processing/#/root/fineweb/base_processing/#g' process_common_crawl_dump.py
sed -i 's#from datatrove.executor.slurm import SlurmPipelineExecutor#from datatrove.executor import LocalPipelineExecutor#g' process_common_crawl_dump.py
sed -i 's#SlurmPipelineExecutor#LocalPipelineExecutor#g' process_common_crawl_dump.py
sed -i 's#Trafilatura(favour_precision=True)#Trafilatura(favour_precision=True, timeout=10.0)#g' process_common_crawl_dump.py
python -c "import nltk; nltk.download('punkt')"
python process_common_crawl_dump.py CC-MAIN-2023-23
# aws s3 ls s3://fineweb-reproduction-20240516/base_processing/
```

ç»“æœå¦‚ä¸‹ï¼š
1. å¯è§ç“¶é¢ˆåœ¨Trafilaturaä¸Šï¼Œå æ®äº†91%çš„æ—¶é—´ã€‚å› ä¸ºTrafilaturaè¿‡æ»¤æ‰äº†å¤§é‡æ–‡æœ¬ï¼Œè¾“å…¥æ–‡æœ¬å¤§å°ä¸º4.72GBï¼Œè€Œè¾“å‡ºæ–‡æœ¬å¤§å°ä»…ä¸º86MBï¼Œä»…ä¿ç•™1.83%çš„æ–‡æœ¬ã€‚
2. å¤„ç†1.2GBå‹ç¼©åçš„CCæ•°æ®é›†ï¼Œè¿™æ˜¯CC-MAIN-2023-23çš„Crawl Archiveï¼ˆå…±æœ‰99ä¸ªArchiveï¼‰ä¸‹çš„1685224653183.5çš„Segmentï¼ˆå…±æœ‰100ä¸ªSegmentï¼‰ï¼Œå•ä¸ªçº¿ç¨‹ï¼Œç”¨æ—¶1156ç§’ï¼Œä¹Ÿå°±æ˜¯1.06MB/sã€‚

```
2024-05-17 06:39:59.728 | SUCCESS  | datatrove.executor.local:run:146 -

ğŸ“‰ğŸ“‰ğŸ“‰ Stats: All 1 tasks ğŸ“‰ğŸ“‰ğŸ“‰

Total Runtime: 19 minutes and 16 seconds

ğŸ“– - READER: ğŸ•· Warc
    Runtime: (2.09%) 24 seconds [0.20 millisecondsÂ±0.66 milliseconds/doc]
    Stats: {input_files: 1, doc_len: 4740085439 [min=1, max=1048576, 138749.10Â±183114/doc], documents: 34162 [34162.00/input_file]}
ğŸ”» - FILTER: ğŸ˜ˆ Url-filter
    Runtime: (0.49%) 5 seconds [0.17 millisecondsÂ±11.14 milliseconds/doc]
    Stats: {total: 34163, forwarded: 33963, doc_len: 4720996535 [min=1, max=1048576, 139004.11Â±183434/doc], dropped: 200, dropped_domain: 101, dropped_hard_blacklisted: 75, dropped_blacklisted_subword: 17, dropped_soft_blacklisted: 6, dropped_subdomain: 1}
ğŸ›¢ - EXTRAC: â› Trafilatura
    Runtime: (91.08%) 17 minutes and 33 seconds [31.02 millisecondsÂ±58.37 milliseconds/doc]
    Stats: {total: 33963, forwarded: 32550, doc_len: 86438862 [min=1, max=601304, 2655.57Â±13256/doc], dropped: 1413}
ğŸ”» - FILTER: ğŸŒ Language ID
    Runtime: (2.01%) 23 seconds [0.71 millisecondsÂ±2.47 milliseconds/doc]
    Stats: {total: 32550, dropped: 20766, forwarded: 11784, doc_len: 30116540 [min=8, max=292206, 2555.71Â±6959/doc]}
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition
    Runtime: (2.70%) 31 seconds [2.65 millisecondsÂ±6.70 milliseconds/doc]
    Stats: {total: 11784, forwarded: 8545, doc_len: 21903496 [min=8, max=131305, 2563.31Â±4958/doc], dropped: 3239, dropped_dup_line_frac: 1387, dropped_duplicated_5_n_grams: 336, dropped_top_3_gram: 210, dropped_top_2_gram: 694, dropped_duplicated_6_n_grams: 24, dropped_duplicated_9_n_grams: 16, dropped_top_4_gram: 383, dropped_duplicated_10_n_grams: 27, dropped_dup_line_char_frac: 133, dropped_duplicated_7_n_grams: 14, dropped_duplicated_8_n_grams: 15}
ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality
    Runtime: (1.44%) 16 seconds [1.95 millisecondsÂ±3.32 milliseconds/doc]
    Stats: {total: 8545, forwarded: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc], dropped: 2474, dropped_gopher_too_many_end_ellipsis: 240, dropped_gopher_short_doc: 1095, dropped_gopher_below_alpha_threshold: 1108, dropped_gopher_enough_stop_words: 20, dropped_gopher_too_many_ellipsis: 2, dropped_gopher_above_avg_threshold: 1, dropped_gopher_too_many_hashes: 2, dropped_gopher_too_many_bullets: 3, dropped_gopher_below_avg_threshold: 3}
ğŸ’½ - WRITER: ğŸ¿ Jsonl
    Runtime: (0.18%) 2 seconds [0.34 millisecondsÂ±0.46 milliseconds/doc]
    Stats: {XXXXX.jsonl.gz: 6071, total: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc]}
```

## å¤ç°Fineweb

```bash
wget https://raw.githubusercontent.com/huggingface/datatrove/main/examples/fineweb.py
sed -i 's/#.*//g' fineweb.py
sed -i 's#from datatrove.executor.slurm import SlurmPipelineExecutor#from datatrove.executor import LocalPipelineExecutor#g' fineweb.py
sed -i 's#CC-MAIN-2O23-5O#CC-MAIN-2023-23#g' fineweb.py
sed -i 's#s3://some_s3_bucket#/root/fineweb#g' fineweb.py
sed -i 's#SlurmPipelineExecutor#LocalPipelineExecutor#g' fineweb.py
sed -i 's#job_name=.*,##g' fineweb.py
sed -i 's#s3://commoncrawl/crawl-data#/root/fineweb/crawl-data#g' fineweb.py
sed -i 's#Trafilatura(favour_precision=True)#Trafilatura(favour_precision=True, timeout=10.0)#g' fineweb.py
sed -i 's#tasks=.*,#tasks=1,#g' fineweb.py
sed -i 's#time=".*",##g' fineweb.py
sed -i 's#slurm_logs_folder=.*",##g' fineweb.py
sed -i 's#randomize_start=.*,##g' fineweb.py
sed -i 's#mem_per_cpu_gb=.*,##g' fineweb.py
sed -i 's#partition=.*,##g' fineweb.py
sed -i 's#logging_dir=.*,##g' fineweb.py
sed -i 's#cpus_per_task=.*,##g' fineweb.py
sed -i 's#depends=.*,##g' fineweb.py
sed -i 's#^stage4.run()#stage1.run(); stage2.run(); stage3.run(); stage4.run()#g' fineweb.py
python -c "import nltk; nltk.download('punkt')"
/usr/bin/time -v python fineweb.py 2>&1 | tee fineweb.log
```

ç»“æœå¦‚ä¸‹

```
--- ğŸ› ï¸ PIPRLINEWğŸ›                                                                                                   [66/1812]
ğŸ“– - READER: ğŸ•· Warc                                                                                                         
ğŸ”» - FILTER: ğŸ˜ˆ Url-filter                                                                                                  
ğŸ›¢ - EXTRAC: â› Trafilatura
ğŸ”» - FILTER: ğŸŒ Language ID
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition                                                                                           
ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality                                                                                              
ğŸ”» - FILTER: â›° C4 Quality                                                                                                   
ğŸ”» - FILTER: ğŸ· FineWeb Quality
ğŸ’½ - WRITER: ğŸ¿ Jsonl
2024-05-17 07:13:38.553 | INFO     | datatrove.pipeline.readers.base:read_files_shard:193 - Reading input file 1685224653183.5/warc/CC-MAIN-20230606214755-20230607004755-00000.warc.gz                                                                 
2024-05-17 07:13:43.012 | WARNING  | datatrove.pipeline.readers.base:get_document_from_dict:93 - Found document without text, skipping. Is your `text_key` ("text") correct?                                                                            
2024-05-17 07:33:20.415 | SUCCESS  | datatrove.executor.base:_run_for_rank:85 - Processing done for rank=0
2024-05-17 07:33:20.417 | INFO     | datatrove.executor.base:_run_for_rank:91 -
                                                                                                                            ğŸ“‰ğŸ“‰ğŸ“‰ Stats: Task 0 ğŸ“‰ğŸ“‰ğŸ“‰
                                                                                                                            Total Runtime: 19 minutes and 27 seconds
                                                                                                                            
ğŸ“– - READER: ğŸ•· Warc
    Runtime: (2.08%) 24 seconds [0.21 millisecondsÂ±0.66 milliseconds/doc]
    Stats: {input_files: 1, doc_len: 4740085439 [min=1, max=1048576, 138749.10Â±183114/doc], documents: 34162 [34162.00/input_file]}                                                                                                                     
ğŸ”» - FILTER: ğŸ˜ˆ Url-filter                                                                                                      
    Runtime: (0.49%) 5 seconds [0.17 millisecondsÂ±11.06 milliseconds/doc]
    Stats: {total: 34163, forwarded: 33963, doc_len: 4720996535 [min=1, max=1048576, 139004.11Â±183434/doc], dropped: 200, dropped_domain: 101, dropped_hard_blacklisted: 75, dropped_blacklisted_subword: 17, dropped_soft_blacklisted: 6, dropped_subdomain: 1}                                                                                                                    
ğŸ›¢ - EXTRAC: â› Trafilatura                                                                                                       
    Runtime: (90.03%) 17 minutes and 31 seconds [30.96 millisecondsÂ±57.26 milliseconds/doc]
    Stats: {total: 33963, forwarded: 32550, doc_len: 86438862 [min=1, max=601304, 2655.57Â±13256/doc], dropped: 1413}
ğŸ”» - FILTER: ğŸŒ Language ID                                                                                                     
    Runtime: (1.99%) 23 seconds [0.72 millisecondsÂ±2.44 milliseconds/doc]                                                       
    Stats: {total: 32550, dropped: 20766, forwarded: 11784, doc_len: 30116540 [min=8, max=292206, 2555.71Â±6959/doc]}
ğŸ”» - FILTER: ğŸ‘¯ Gopher Repetition
    Runtime: (2.66%) 31 seconds [2.64 millisecondsÂ±6.71 milliseconds/doc]
    Stats: {total: 11784, forwarded: 8545, doc_len: 21903496 [min=8, max=131305, 2563.31Â±4958/doc], dropped: 3239, dropped_dup_line_frac: 1387, dropped_duplicated_5_n_grams: 336, dropped_top_3_gram: 210, dropped_top_2_gram: 694, dropped_duplicated_6_n_grams: 24, dropped_duplicated_9_n_grams: 16, dropped_top_4_gram: 383, dropped_duplicated_10_n_grams: 27, dropped_dup_line_char_frac: 133, dropped_duplicated_7_n_grams: 14, dropped_duplicated_8_n_grams: 15}
ğŸ”» - FILTER: ğŸ¥‡ Gopher Quality                                                                                                  
    Runtime: (1.42%) 16 seconds [1.94 millisecondsÂ±3.30 milliseconds/doc]
    Stats: {total: 8545, forwarded: 6071, doc_len: 19018684 [min=254, max=131305, 3132.71Â±5443/doc], dropped: 2474, dropped_gopher_too_many_end_ellipsis: 240, dropped_gopher_short_doc: 1095, dropped_gopher_below_alpha_threshold: 1108, dropped_gopher_enough_stop_words: 20, dropped_gopher_too_many_ellipsis: 2, dropped_gopher_above_avg_threshold: 1, dropped_gopher_too_many_hashes: 2, dropped_gopher_too_many_bullets: 3, dropped_gopher_below_avg_threshold: 3}
ğŸ”» - FILTER: â›° C4 Quality
    Runtime: (0.32%) 3 seconds [0.62 millisecondsÂ±1.02 milliseconds/doc]
    Stats: {total: 6071, line-total: 123281, line-kept: 97170, dropped: 602, dropped_too_few_sentences: 573, line-filter-too_few_words: 25349, forwarded: 5469, doc_len: 17992639 [min=149, max=123528, 3289.93Â±5383/doc], line-filter-policy: 641, dropped_curly_bracket: 22, line-filter-javascript: 92, dropped_lorem_ipsum: 7}
ğŸ”» - FILTER: ğŸ· FineWeb Quality
    Runtime: (0.85%) 9 seconds [1.81 millisecondsÂ±2.58 milliseconds/doc]
    Stats: {total: 5469, dropped: 647, dropped_line_punct_ratio: 336, forwarded: 4822, doc_len: 15898156 [min=151, max=99375, 3297.00Â±4732/doc], dropped_short_line_ratio: 86, dropped_char_dup_ratio: 225}
ğŸ’½ - WRITER: ğŸ¿ Jsonl
    Runtime: (0.15%) 1 second [0.37 millisecondsÂ±0.45 milliseconds/doc]
    Stats: {XXXXX.jsonl.gz: 4822, total: 4822, doc_len: 15898156 [min=151, max=99375, 3297.00Â±4732/doc]}
2024-05-17 07:33:20.423 | SUCCESS  | datatrove.executor.local:run:146 -

ğŸ“‰ğŸ“‰ğŸ“‰ Stats: All 1 tasks ğŸ“‰ğŸ“‰ğŸ“‰

Total Runtime: 19 minutes and 27 seconds

        Command being timed: "python fineweb.py"
        User time (seconds): 1177.85
        System time (seconds): 2.05
        Percent of CPU this job got: 99%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 19:42.98
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 1029440
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 1
        Minor (reclaiming a frame) page faults: 342151
        Voluntary context switches: 71803
        Involuntary context switches: 3084
        Swaps: 0
        File system inputs: 0
        File system outputs: 84208
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 1
```

# æ€§èƒ½åˆ†æ

ç»™`WarcReader`åŠ ä¸€ä¸ª`limit=1000`å³å¯åœ¨26ç§’å†…å‡ºç»“æœã€‚

```bash
python -m cProfile -o process_common_crawl_dump.cProfile -s cumtime process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.cProfile
```

çœ‹ä¸Šå»cPythonå¯¹å¤šçº¿ç¨‹çš„æ”¯æŒæœ‰é™ï¼Œ[extractorä½¿ç”¨äº†ThreadPoolExecutoræ¥åšåŸºäºå¤šçº¿ç¨‹çš„çº¿ç¨‹æ± ](https://github.com/huggingface/datatrove/blob/main/src/datatrove/pipeline/extractors/base.py#L48)ã€‚

å°è¯•`yaapi`ï¼Œå¯ä»¥è‡ªåº•å‘ä¸Šçœ‹åˆ°å…·ä½“å“ªäº›è°ƒç”¨æ˜¯çƒ­ç‚¹ã€‚ä¸è¿‡å®ƒå­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåœ¨ä½¿ç”¨yappiå¯¹Cythonæ¨¡å—è¿›è¡Œåˆ†ææ—¶ï¼ŒCythonå‡½æ•°çš„è°ƒç”¨ç»Ÿè®¡é€šå¸¸ä¼šå½’å…¥è°ƒç”¨å®ƒä»¬çš„Pythonå‡½æ•°ã€‚

```bash
pip install yappi
python -m yappi -c cpu -o process_common_crawl_dump.yappi -f pstat process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.yappi
# sort tottime
# stats 100

# è‹¥æ²¡æœ‰-bï¼Œåˆ™ä¸ä¼šå°†builtinå‡½æ•°è€ƒè™‘è¿›å»ï¼Œæ‰€ä»¥æ‰“å¼€-bè¿›ä¸€æ­¥çœ‹çœ‹å“ªäº›builtinæ˜¯æœ€éœ€è¦ä¼˜åŒ–çš„
python -m yappi -c cpu -b -o process_common_crawl_dump.yappi.withBuiltins -f pstat process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump.yappi.withBuiltins
# sort tottime
# stats 100

# åšå…¨é‡æ•°æ®åˆ†æ
rm -rf logging; python -m yappi -c cpu -b -o process_common_crawl_dump-FULL.yappi -f pstat process_common_crawl_dump.py CC-MAIN-2023-23; rm -rf logging; py-spy record -o process_common_crawl_dump-FULL.pyspy.svg -f flamegraph --rate 100 -- python process_common_crawl_dump.py CC-MAIN-2023-23
python -m pstats process_common_crawl_dump-FULL.yappi
# sort tottime
# stats 100
```

ä½¿ç”¨`py-spy`ç”»ç«ç„°å›¾ï¼Œå¯ä»¥è‡ªé¡¶å‘ä¸‹åšæ€§èƒ½åˆ†æã€‚[py-spyé»˜è®¤æ˜¯CPUæ¨¡å¼ï¼Œåœ¨åŠ ä¸Š--idleå‘½ä»¤ä¹‹åæ˜¯Wallæ¨¡å¼](https://github.com/benfred/py-spy/issues/458)ï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹CPUæ¨¡å¼ï¼š


```bash
pip install py-spy
py-spy record -o process_common_crawl_dump.pyspy.svg -f flamegraph --rate 100 -- python process_common_crawl_dump.py CC-MAIN-2023-23
```

å°è¯•`line_profiler`ï¼š

```bash
pip install line_profiler
vi pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
# åœ¨def prune_unwanted_nodesä¹‹å‰å¢åŠ  @line_profiler.profile
LINE_PROFILE=1 python process_common_crawl_dump.py CC-MAIN-2023-23
python -m line_profiler -rtmz profile_output.lprof
```

è¾“å‡ºç»“æœï¼š
```
 11.13 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
Wrote profile results to profile_output.txt
Wrote profile results to profile_output_2024-05-19T081609.txt
Wrote profile results to profile_output.lprof
To view details run:
python -m line_profiler -rtmz profile_output.lprof
```

è¿›ä¸€æ­¥æŸ¥çœ‹
```
Timer unit: 1e-06 s

Total time: 11.1257 s
File: /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
Function: prune_unwanted_nodes at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @line_profiler.profile
    91                                           def prune_unwanted_nodes(tree, nodelist, with_backup=False):
    92                                               '''Prune the HTML tree by removing unwanted sections.'''
    93     11665       3130.6      0.3      0.0      if with_backup:
    94      1577      25789.5     16.4      0.2          old_len = len(tree.text_content())  # ' '.join(tree.itertext())
    95      1577      77741.7     49.3      0.7          backup = deepcopy(tree)
    96                                           
    97     27462       6424.3      0.2      0.1      for expression in nodelist:
    98     54290   10884183.9    200.5     97.8          for subtree in expression(tree):
    99                                                       # preserve tail text from deletion
   100     38493       8026.8      0.2      0.1              if subtree.tail is not None:
   101     28309      10939.3      0.4      0.1                  prev = subtree.getprevious()
   102     28309       3699.4      0.1      0.0                  if prev is None:
   103     17852       6883.3      0.4      0.1                      prev = subtree.getparent()
   104     28309       3616.2      0.1      0.0                  if prev is not None:
   105                                                               # There is a previous node, append text to its tail
   106     28309      23310.3      0.8      0.2                      prev.tail = " ".join([prev.tail, subtree.tail]) if prev.tail else subtree.tail
   107                                                       # remove the node
   108     38493      52627.2      1.4      0.5              subtree.getparent().remove(subtree)
   109                                           
   110     11665       1707.4      0.1      0.0      if not with_backup:
   111     10088       1384.7      0.1      0.0          return tree
   112                                           
   113      1577      14788.8      9.4      0.1      new_len = len(tree.text_content())
   114                                               # todo: adjust for recall and precision settings
   115      1577       1195.3      0.8      0.0      if new_len > old_len/7:
   116      1444        198.0      0.1      0.0          return tree
   117       133         17.6      0.1      0.0      return backup

 11.13 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
 ```

 æ¨æµ‹æ˜¯`expression(tree)`è°ƒç”¨æ—¶é—´æœ€é•¿ï¼ŒæŠŠå®ƒä»forå¾ªç¯é‡Œæ‹†å¼€æ¥åè¿›ä¸€æ­¥åˆ†æï¼Œæœç„¶`XPath`æ±‚å€¼æ˜¯æœ€æ…¢çš„ï¼Œå äº†97.1%çš„æ—¶é—´ã€‚

 ```
 Timer unit: 1e-06 s

Total time: 11.1063 s
File: /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py
Function: prune_unwanted_nodes at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @line_profiler.profile
    91                                           def prune_unwanted_nodes(tree, nodelist, with_backup=False):
    92                                               '''Prune the HTML tree by removing unwanted sections.'''
    93     11665       3206.3      0.3      0.0      if with_backup:
    94      1577      25812.4     16.4      0.2          old_len = len(tree.text_content())  # ' '.join(tree.itertext())
    95      1577      78249.5     49.6      0.7          backup = deepcopy(tree)
    96                                           
    97     27462       6485.1      0.2      0.1      for expression in nodelist:
    98     15797   10848708.6    686.8     97.7          subtrees = expression(tree)
    99     54290      13832.9      0.3      0.1          for subtree in subtrees:
   100                                                       # preserve tail text from deletion
   101     38493       7947.7      0.2      0.1              if subtree.tail is not None:
   102     28309      11156.7      0.4      0.1                  prev = subtree.getprevious()
   103     28309       3635.4      0.1      0.0                  if prev is None:
   104     17852       7424.6      0.4      0.1                      prev = subtree.getparent()
   105     28309       3392.5      0.1      0.0                  if prev is not None:
   106                                                               # There is a previous node, append text to its tail
   107     28309      23303.6      0.8      0.2                      prev.tail = " ".join([prev.tail, subtree.tail]) if prev.tail else subtree.tail
   108                                                       # remove the node
   109     38493      52844.9      1.4      0.5              subtree.getparent().remove(subtree)
   110                                           
   111     11665       1721.9      0.1      0.0      if not with_backup:
   112     10088       1288.3      0.1      0.0          return tree
   113                                           
   114      1577      15838.1     10.0      0.1      new_len = len(tree.text_content())
   115                                               # todo: adjust for recall and precision settings
   116      1577       1234.6      0.8      0.0      if new_len > old_len/7:
   117      1444        200.1      0.1      0.0          return tree
   118       133         17.2      0.1      0.0      return backup

 11.11 seconds - /root/pyvenv-fineweb/lib/python3.10/site-packages/trafilatura/htmlprocessing.py:90 - prune_unwanted_nodes
 ```

## åˆ†æç»“æœ

é€šè¿‡`yappi`çš„ç»“æœå‘ç°`prune_unwanted_nodes`åœ¨Pythonè§£é‡Šå™¨çš„éƒ¨åˆ†å ç”¨äº†ä¸å°‘æ—¶é—´ï¼Œ
ä½†æ˜¯ç”±äºå†å¾€ä¸‹æ˜¯ç”¨Cythonå®ç°çš„ï¼Œæ²¡æ³•å†è¿›ä¸€æ­¥åˆ†è§£äº†ã€‚
å°è¯•ç”¨line_profilerè¿›è¡Œåˆ†æã€‚

æ€»å¾—æ¥è¯´ï¼Œå¯ä»¥ç”¨`yappi`/`py-spy`å®šä½åˆ°çƒ­ç‚¹Pythonå‡½æ•°ï¼Œå¦‚æœèƒ½æ‰¾åˆ°çƒ­ç‚¹å‡½æ•°ï¼Œå¯ä»¥å†ç”¨`line_profiler`è¿›ä¸€æ­¥ç»†åŒ–çƒ­ç‚¹æƒ…å†µã€‚

# lxmlä»æºç æ„å»º

```bash
git clone https://github.com/lxml/lxml
cd lxml
git checkout lxml-5.2.2
sudo apt install libxslt-dev libxml2-dev
pip install -r requirements.txt

# å°±åœ°æ„å»ºCythonç›¸å…³çš„ä¾èµ–
python setup.py build_ext -i --with-cython

# æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œ1946ä¸ªæµ‹è¯•å…¨éƒ¨é€šè¿‡
python test.py

python ./benchmark/bench_xpath.py
```

å…±è®¡æœ‰4ä¸ªCythonæ–‡ä»¶ï¼Œåˆ†åˆ«æ˜¯`src/lxml/etree.pyx`ã€`src/lxml/objectify.pyx`ã€`src/lxml/builder.py`ã€`src/lxml/_elementpath.py`ã€`src/lxml/html/diff.py`ã€`src/lxml/sax.py`ã€‚
å¯¹åº”çš„Cæ–‡ä»¶åˆ†åˆ«åœ¨`src/lxml/etree.c`ã€`src/lxml/objectify.c`ã€`src/lxml/builder.c`ã€`src/lxml/_elementpath.c`ã€`src/lxml/html/diff.c`ã€`src/lxml/sax.c`ã€‚

`XPath`ç±»çš„å®šä¹‰ä½äºxpath.pxi:377ï¼Œå®ƒé¦–å…ˆä¼šè°ƒç”¨`xmlXPathCtxtCompile`å°†XPathæŸ¥è¯¢ç¼–è¯‘ï¼Œè¿›è€Œè°ƒç”¨`xmlXPathCompiledEval`ï¼ˆå£°æ˜è‡ªxpath.pyd:85ï¼Œå®ç°åœ¨libxml2ï¼Œå¯è§xpath.pxdçš„ä½œç”¨æ˜¯libxml2çš„Cython-bindingï¼‰å»æœç´¢XMLæ ‘ã€‚

# libxml2ä»æºä»£ç æ„å»º

```bash
sudo apt install autoconf libtool bear
git clone https://github.com/GNOME/libxml2
# æŸ¥è¯¢åˆ°ç‰ˆæœ¬ä¸º2.9.13+dfsg-1ubuntu0.4
dpkg -l | grep libxml2
git checkout v2.9.13
CFLAGS='-O2 -fno-semantic-interposition' ./autogen.sh
bear -- make
make runtest
./runtest
make testXPath && LD_LIBRARY_PATH=.libs ./.libs/testXPath -f example_expression.txt
```

XPathæœ‰ä»¥ä¸‹çš„OPï¼š
1. ENDï¼šç»“æŸç¨‹åº
2. ANDï¼šäºŒå…ƒæ“ä½œï¼Œå¸ƒå°”å€¼ä¸
3. ORï¼šäºŒå…ƒæ“ä½œï¼Œå¸ƒå°”å€¼æˆ–
4. EQUALï¼šäºŒå…ƒæ“ä½œï¼Œåˆ¤å®šç›¸ç­‰
5. CMPï¼šäºŒå…ƒæ“ä½œï¼Œæ¯”è¾ƒï¼ˆå¤§äºã€å°äºï¼‰
6. PLUSï¼šäºŒå…ƒæ“ä½œï¼ŒåŠ æ³•
7. MULTï¼šäºŒå…ƒæ“ä½œï¼Œä¹˜æ³•
8. UNIONï¼šäºŒå…ƒæ“ä½œï¼Œé¢å‘NodeSetï¼Œå°†ç¬¬äºŒä¸ªå‚æ•°çš„Nodeåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªå‚æ•°çš„NodeSetä¸­
9. ROOTï¼šåˆå§‹åŒ–Value Stackï¼Œå°†documentçš„rootåŠ å…¥NodeSet
10. NODEï¼šå°†0ã€1ã€2ä¸ªå‚æ•°æ±‚å€¼åï¼Œå°†åŒ…å«ä¸Šä¸‹æ–‡Nodeçš„å•å…ƒç´ NodeSetæ¨å…¥æ ˆé¡¶
11. COLLECT
12. VALUEï¼šå°†opçš„value4ï¼ˆç¬¬ä¸€ä¸ªæŒ‡é’ˆå‚æ•°ï¼‰æ‹·è´ä¸€ä»½å¹¶æ¨å…¥æ ˆé¡¶
13. VARIABLEï¼šæŸ¥æ‰¾å˜é‡è¡¨ï¼ˆä¸€ä¸ªå‚æ•°ä¸ºå•ä¸€åå­—ï¼Œä¸¤ä¸ªå‚æ•°ä¸ºå±€éƒ¨åå­—+å‘½åç©ºé—´ï¼‰
14. FUNCTIONï¼šè°ƒç”¨å‡½æ•°
15. ARG
16. PREDICATE
17. FILTER
18. SORT
19. RANGETO

## Collectæ“ä½œ

å®ƒçš„collectæ“ä½œæ¯”è¾ƒå¤æ‚ï¼Œæœ‰å¤šä¸ªå‚æ•°

```c
typedef enum {
    AXIS_ANCESTOR = 1,
    AXIS_ANCESTOR_OR_SELF,
    AXIS_ATTRIBUTE,
    AXIS_CHILD,
    AXIS_DESCENDANT,
    AXIS_DESCENDANT_OR_SELF,
    AXIS_FOLLOWING,
    AXIS_FOLLOWING_SIBLING,
    AXIS_NAMESPACE,
    AXIS_PARENT,
    AXIS_PRECEDING,
    AXIS_PRECEDING_SIBLING,
    AXIS_SELF
} xmlXPathAxisVal;
```

```c
typedef enum {
    NODE_TEST_NONE = 0,
    NODE_TEST_TYPE = 1,
    NODE_TEST_PI = 2,
    NODE_TEST_ALL = 3,
    NODE_TEST_NS = 4,
    NODE_TEST_NAME = 5
} xmlXPathTestVal;
```

```c
typedef enum {
    NODE_TYPE_NODE = 0,
    NODE_TYPE_COMMENT = XML_COMMENT_NODE,
    NODE_TYPE_TEXT = XML_TEXT_NODE,
    NODE_TYPE_PI = XML_PI_NODE
} xmlXPathTypeVal;
```

## å…¶ä»–


ç¼–è¯‘åçš„è¡¨è¾¾å¼ç»“æ„ä½“å¦‚ä¸‹ï¼š

```c
struct _xmlXPathCompExpr {
    int nbStep;			/* Number of steps in this expression */
    int maxStep;		/* Maximum number of steps allocated */
    xmlXPathStepOp *steps;	/* ops for computation of this expression */
    int last;			/* index of last step in expression */
    xmlChar *expr;		/* the expression being computed */
    xmlDictPtr dict;		/* the dictionary to use if any */
#ifdef DEBUG_EVAL_COUNTS
    int nb;
    xmlChar *string;
#endif
#ifdef XPATH_STREAMING
    xmlPatternPtr stream;
#endif
};
```

ç¼–è¯‘åçš„è¡¨è¾¾å¼æ˜¯ä¸€ä¸ªæ ‘å½¢ç»“æ„ï¼ˆæœ€å¤šæœ‰ä¸¤ä¸ªå„¿å­ï¼‰ï¼Œè§£é‡Šæ‰§è¡Œä»`comp->last`æŒ‡å‘çš„æœ€åä¸€ä¸ªæ“ä½œå¼€å§‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæœ€åä¸€ä¸ªæ“ä½œæ˜¯æ ‘æ ¹ã€‚
ä»¥ä¸€ç§é€’å½’çš„æ–¹å¼è§£é‡Šæ‰§è¡Œä»»æ„ä¸€æ£µå­æ ‘ã€‚

æ‰§è¡Œç¯å¢ƒæ˜¯ä¸€ä¸ªå †æ ˆæœºï¼Œä¸Šä¸‹æ–‡é‡Œæœ‰ä¸€ä¸ªStackã€‚

```c
struct _xmlXPathParserContext {
    const xmlChar *cur;			/* the current char being parsed */
    const xmlChar *base;		/* the full expression */

    int error;				/* error code */

    xmlXPathContextPtr  context;	/* the evaluation context */
    xmlXPathObjectPtr     value;	/* ç»´æŠ¤äº†Stack topï¼ˆViewï¼‰ */
    int                 valueNr;	/* å½“å‰Stackå¤§å° */
    int                valueMax;	/* å½“å‰Stackçš„å®¹é‡ */
    xmlXPathObjectPtr *valueTab;	/* Stackæ•°æ®ç»“æ„ */

    xmlXPathCompExprPtr comp;		/* the precompiled expression */
    int xptr;				/* it this an XPointer expression */
    xmlNodePtr         ancestor;	/* used for walking preceding axis */

    int              valueFrame;        /* å½“å‰Frameå…è®¸çš„æœ€å°Stacké•¿åº¦ï¼ˆé¿å…å½±å“è°ƒç”¨è€…çš„å€¼ï¼‰ */
};
```

é™¤äº†Value Stackä¹‹å¤–è¿˜æœ‰ä¸€ä¸ªEvaluation Contextï¼š

```c
struct _xmlXPathContext {
    xmlDocPtr doc;			/* å½“å‰æ­£åœ¨å¤„ç†çš„documentï¼Ÿ */
    xmlNodePtr node;			/* å½“å‰æ­£åœ¨å¤„ç†çš„nodeï¼Ÿ */

    int nb_variables_unused;		/* unused (hash table) */
    int max_variables_unused;		/* unused (hash table) */
    xmlHashTablePtr varHash;		/* Hash table of defined variables */

    int nb_types;			/* number of defined types */
    int max_types;			/* max number of types */
    xmlXPathTypePtr types;		/* Array of defined types */

    int nb_funcs_unused;		/* unused (hash table) */
    int max_funcs_unused;		/* unused (hash table) */
    xmlHashTablePtr funcHash;		/* Hash table of defined funcs */

    int nb_axis;			/* number of defined axis */
    int max_axis;			/* max number of axis */
    xmlXPathAxisPtr axis;		/* Array of defined axis */

    /* the namespace nodes of the context node */
    xmlNsPtr *namespaces;		/* Array of namespaces */
    int nsNr;				/* number of namespace in scope */
    void *user;				/* function to free */

    /* extra variables */
    int contextSize;			/* the context size */
    int proximityPosition;		/* the proximity position */

    /* extra stuff for XPointer */
    int xptr;				/* is this an XPointer context? */
    xmlNodePtr here;			/* for here() */
    xmlNodePtr origin;			/* for origin() */

    /* the set of namespace declarations in scope for the expression */
    xmlHashTablePtr nsHash;		/* The namespaces hash table */
    xmlXPathVariableLookupFunc varLookupFunc;/* variable lookup func */
    void *varLookupData;		/* variable lookup data */

    /* Possibility to link in an extra item */
    void *extra;                        /* needed for XSLT */

    /* The function name and URI when calling a function */
    const xmlChar *function;
    const xmlChar *functionURI;

    /* function lookup function and data */
    xmlXPathFuncLookupFunc funcLookupFunc;/* function lookup func */
    void *funcLookupData;		/* function lookup data */

    /* temporary namespace lists kept for walking the namespace axis */
    xmlNsPtr *tmpNsList;		/* Array of namespaces */
    int tmpNsNr;			/* number of namespaces in scope */

    /* error reporting mechanism */
    void *userData;                     /* user specific data block */
    xmlStructuredErrorFunc error;       /* the callback in case of errors */
    xmlError lastError;			/* the last error */
    xmlNodePtr debugNode;		/* the source node XSLT */

    /* dictionary */
    xmlDictPtr dict;			/* dictionary if any */

    int flags;				/* flags to control compilation */

    /* Cache for reusal of XPath objects */
    void *cache;

    /* Resource limits */
    unsigned long opLimit;
    unsigned long opCount;
    int depth;
};
```

å…·ä½“æ¯ä¸ªæ“ä½œç»“æ„ä½“å¦‚ä¸‹ï¼š

```c
struct _xmlXPathStepOp {
    xmlXPathOp op;		/* æ“ä½œç¼–å· */
    int ch1;			/* ç¬¬ä¸€ä¸ªå„¿å­ */
    int ch2;			/* ç¬¬äºŒä¸ªå„¿å­ */
    int value;
    int value2;
    int value3;
    void *value4;
    void *value5;
    xmlXPathFunction cache;
    void *cacheURI;
};
```

# ç›¸å…³å¼€æºæ•°æ®é›†

1. [RefinedWeb](https://huggingface.co/papers/2306.01116)ï¼šæœªå¼€æº
2. [C4](https://www.tensorflow.org/datasets/catalog/c4)ï¼šè°·æ­ŒåŸºäºCommon Crawlæ¸…æ´—å‡ºæ¥çš„æ•°æ®é›†
3. 

# ç›¸å…³å¼€æºåº“

1. [datatrove](https://github.com/huggingface/datatrove)ï¼šHugging Faceå®˜æ–¹æ¨å‡ºçš„åŸºäºSlurmçš„æ•°æ®å¤„ç†åº“ã€‚
2. 

# Hugging Faceå®˜æ–¹çš„åº“

1. [transformers](https://github.com/huggingface/transformers)ï¼šä¸šç•Œé¢†å…ˆçš„é¢å‘JAXã€PyTorchã€TensorFlowçš„æœºå™¨å­¦ä¹ ã€‚
2. [diffusers](https://github.com/huggingface/diffusers)ï¼šä¸šç•Œé¢†å…ˆçš„ç”¨äºå›¾åƒã€éŸ³é¢‘ç”Ÿæˆçš„æ¨¡å‹ï¼ˆåŸºäºPyTorchå’ŒFLAXï¼‰ã€‚
3. [dataset](https://github.com/huggingface/datasets)ï¼šé¢å‘æœºå™¨å­¦ä¹ è®­ç»ƒçš„æ•°æ®é›†ï¼Œä»¥åŠæ˜“ç”¨ã€é«˜æ•ˆçš„æ•°æ®å¤„ç†å·¥å…·ã€‚
4. [peft](https://github.com/huggingface/peft)ï¼šä¸šç•Œé¢†å…ˆçš„å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ŒPEFTæ–¹æ³•åªéœ€è¦fine-tuningä¸€å°éƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„æ¨¡å‹å‚æ•°ã€‚
5. [accelerate](https://github.com/huggingface/accelerate)ï¼šè¿è¡Œã€è®­ç»ƒã€ä½¿ç”¨PyTorchæ¨¡å‹åœ¨ä»»ä½•è®¾å¤‡ã€åˆ†å¸ƒå¼é…ç½®ä¸‹ã€‚è‡ªåŠ¨æ–½åŠ æ··åˆç²¾åº¦ã€‚
6. [optimum](https://github.com/huggingface/optimum)ï¼šåŠ é€ŸTransformerä¸Diffusersçš„è®­ç»ƒä¸æ¨ç†ï¼Œé¢å‘å„ç§ç¡¬ä»¶ä¼˜åŒ–å·¥å…·ã€‚

